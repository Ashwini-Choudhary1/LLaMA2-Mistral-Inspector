# LLaMA2-Mistral-Inspector (Apple Silicon M1)

Task-aware benchmarking and failure analysis of large language models, with a
focus on comparing **Mistral** and **LLaMA-2** under identical constraints.

This project evaluates model behavior across:
- Question Answering (QA)
- Text Summarization
- Logical Reasoning

Key features include:
- Task-aware evaluation metrics
- Interpretable failure analysis
- CPU-only inference using `llama.cpp` (tested on Apple Silicon M1)
- Deterministic benchmarking to expose systematic model behavior

More features and experiments are actively being added.
